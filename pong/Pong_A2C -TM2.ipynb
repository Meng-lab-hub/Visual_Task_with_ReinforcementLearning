{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the necessary software libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Todos: Save the best model (highest reward)\n",
    "#        Save reward for every episode to the file\n",
    "#    create python script\n",
    "#    need only 2 layers in actor and critic\n",
    "#    read how to access GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch import nn as nn\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from utils import test_policy_network, seed_everything, plot_stats\n",
    "from parallel_env import ParallelEnv, ParallelWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import ROM from the extracted file to atari_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for gym 0.20+\n",
    "# from ale_py import ALEInterface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for gym 0.20+\n",
    "# ale = ALEInterface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for gym 0.20+\n",
    "# ! ale-import-roms /opt/anaconda3/envs/pongA2C/lib/python3.8/site-packages/ale_py/roms/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run only once when creating conda environment\n",
    "! python -m atari_py.import_roms /Users/meng/Downloads/Roms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for gym 0.20+\n",
    "# from ale_py.roms import Pong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for gym 0.20+\n",
    "# ale.loadROM(Pong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need to run?\n",
    "import atari_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyglet==1.5.27 # added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torchvision # added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'Pong-v4'\n",
    "#env_name = 'Acrobot-v1'\n",
    "\n",
    "env = gym.make(env_name)\n",
    "\n",
    "input_space = env.observation_space.shape\n",
    "\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"State dimensions: {input_space}. Actions: {actions}\")\n",
    "print(f\"Sample state: {env.reset()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"state_space[0] = {input_space[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.real(env.render(mode='rgb_array')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "\n",
    "    def __init__(self, env_name):\n",
    "        self.env = gym.make(env_name)\n",
    "        seed_everything(self.env)\n",
    "\n",
    "    def reset(self):\n",
    "        obs = self.env.reset()\n",
    "        return obs\n",
    "\n",
    "    def step_async(self, actions):\n",
    "        actions = actions.squeeze().numpy()\n",
    "        self.env.step_async(actions)\n",
    "\n",
    "    def step_wait(self):\n",
    "        obs, reward, done, info = self.env.step_wait()\n",
    "        obs = self.preprocess_observation(obs)\n",
    "        reward = torch.tensor(reward).unsqueeze(1).float()\n",
    "        done = torch.tensor(done).unsqueeze(1)\n",
    "        return obs, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessEnv(ParallelWrapper):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def reset(self):\n",
    "        state = self.venv.reset()\n",
    "        return torch.from_numpy(state).float()\n",
    "\n",
    "    def step_async(self, actions):\n",
    "        actions = actions.squeeze().numpy()\n",
    "        self.venv.step_async(actions)\n",
    "\n",
    "    def step_wait(self):\n",
    "        next_state, reward, done, info = self.venv.step_wait()\n",
    "        next_state = torch.from_numpy(next_state).float()\n",
    "        reward = torch.tensor(reward).unsqueeze(1).float()\n",
    "        done = torch.tensor(done).unsqueeze(1)\n",
    "        return next_state, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "\n",
    "    def __init__(self, input_shape):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv4 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64*4*4, 512)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        \n",
    "        x = x.view(-1, 64*4*4)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "\n",
    "    def __init__(self, n_actions):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=512, out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=512, out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic():\n",
    "    def __init__(self, actor, critic, feature, alpha=1e-4, gamma=0.99, obs_shape=(1, 64, 64)):\n",
    "        self.actor = actor\n",
    "        self.critic = critic\n",
    "        self.feature = feature\n",
    "        self.obs_shape = obs_shape\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.actor_optim = AdamW(self.actor.parameters(), lr=1e-3)\n",
    "        self.critic_optim = AdamW(self.critic.parameters(), lr=1e-4)\n",
    "        self.feature_optim = AdamW(self.feature.parameters(), lr=1e-4)\n",
    "        self.stats = {'Actor Loss': [], 'Critic Loss': [], 'Returns': []}\n",
    "        \n",
    "    def preprocess_observation(self, obs_batch):\n",
    "        # Crop the score and border region\n",
    "        obs_batch = obs_batch[:, 35:195, :, :]\n",
    "    \n",
    "        # Resize to 64x64\n",
    "        transform = transforms.Resize((64, 64))\n",
    "        \n",
    "        obs_batch = torch.stack([transform(obs.permute(2, 0, 1)) for obs in obs_batch])\n",
    "        \n",
    "        # Convert to float and rescale to range [0, 1]\n",
    "        obs_batch = obs_batch / 255.0\n",
    "        \n",
    "        return obs_batch\n",
    "\n",
    "    def train(self, env, episodes):\n",
    "        for episode in tqdm(range(1, episodes + 1)):\n",
    "            state = env.reset()\n",
    "            state = self.preprocess_observation(state)\n",
    "            \n",
    "            done_b = torch.zeros((env.num_envs, 1), dtype=torch.bool)\n",
    "            ep_return = torch.zeros((env.num_envs, 1))\n",
    "            I = 1.\n",
    "\n",
    "            while not done_b.all():\n",
    "                \n",
    "                self.critic_optim.zero_grad()\n",
    "                self.actor_optim.zero_grad()\n",
    "\n",
    "                '''\n",
    "                    I wanna do something like state = self.feature(state). of course I also change everywhere\n",
    "                in the code but I got error.\n",
    "                    first iteration state.shape = [8, 1, 64, 64] as input\n",
    "                    second iteration state.shape = [8, 512] as input\n",
    "                '''\n",
    "                state_f = self.feature(state)\n",
    "                probs = self.actor(state_f)\n",
    "                \n",
    "                action = torch.multinomial(probs, 1).squeeze().detach()\n",
    "                \n",
    "\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                next_state = self.preprocess_observation(next_state)\n",
    "                \n",
    "                value = self.critic(state_f)\n",
    "                next_state_f = self.feature(next_state)\n",
    "                \n",
    "                target = reward + ~done * self.gamma * self.critic(next_state_f).detach()\n",
    "                critic_loss = nn.functional.mse_loss(value, target)\n",
    "                \n",
    "                \n",
    "                advantage = (target - value).detach()\n",
    "                #probs = self.actor(state)\n",
    "                log_probs = torch.log(probs + 1e-6)\n",
    "                \n",
    "                action = action.view(-1, 1)\n",
    "                \n",
    "                action_log_prob = log_probs.gather(1, action)\n",
    "                entropy = - torch.sum(probs * log_probs, dim=-1, keepdim=True)\n",
    "                actor_loss = - I * action_log_prob * advantage - 0.01 * entropy\n",
    "                actor_loss = actor_loss.mean()\n",
    "                \n",
    "                total_loss = actor_loss + critic_loss\n",
    "                \n",
    "                total_loss.backward()\n",
    "                self.feature_optim.step()\n",
    "                self.critic_optim.step()\n",
    "                self.actor_optim.step()\n",
    "\n",
    "                ep_return += reward\n",
    "                done_b |= done\n",
    "                state = next_state\n",
    "                I = I * self.gamma\n",
    "\n",
    "            self.stats['Actor Loss'].append(actor_loss.item())\n",
    "            self.stats['Critic Loss'].append(critic_loss.item())\n",
    "            self.stats['Returns'].append(ep_return.mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'Pong-v4'\n",
    "num_envs = os.cpu_count()\n",
    "episodes = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = ParallelEnv([lambda: Environment(env_name).env for _ in range(num_envs)])\n",
    "envs = PreprocessEnv(envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = FeatureExtractor(envs.observation_space.shape)\n",
    "actor = Actor(envs.action_space.n)\n",
    "critic = Critic()\n",
    "agent = ActorCritic(actor, critic, feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train(envs, episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stats(agent.stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_name)\n",
    "test_policy_network_internal(env, agent.actor, agent.feature_extractor, episodes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_policy_network_internal(env, policy, feature_extractor, episodes=1):\n",
    "    from IPython import display\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        state = preprocess(state)\n",
    "        state = agent.feature_extractor(state)\n",
    "        done = False\n",
    "        img = plt.imshow(env.render(mode='rgb_array'))\n",
    "        while not done:\n",
    "            # state = torch.from_numpy(state).unsqueeze(0).float()\n",
    "            action = policy(state).multinomial(1).item()\n",
    "            next_state, _, done, _ = env.step(action)\n",
    "            img.set_data(env.render(mode='rgb_array'))\n",
    "            plt.axis('off')\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "            state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(obs):\n",
    "        # Convert to float and rescale to range [0, 1]\n",
    "        #obs_batch = obs_batch.astype(np.float32) / 255.0\n",
    "        \n",
    "        obs = torch.from_numpy(obs)\n",
    "    \n",
    "        # Crop the score and border region\n",
    "        obs = obs[35:195, :, :]\n",
    "    \n",
    "        # Resize to 64x64\n",
    "        transform = transforms.Resize((64, 64))\n",
    "        obs = transform(obs.permute(2, 0, 1))\n",
    "        obs = obs.float()\n",
    "    \n",
    "        return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "state = preprocess(state)\n",
    "state = feature_extractor(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = feature_extractor(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = actor(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_observation(obs):\n",
    "    # selecting the row from 35 to 195 so that we have a space of 160 * 160\n",
    "    obs = obs[35:195, :, :]\n",
    "    # change to gray scale\n",
    "    obs = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)\n",
    "    # resize \n",
    "    obs = cv2.resize(obs, (obs_shape[1], obs_shape[2]), interpolation=cv2.INTER_AREA)\n",
    "    obs = obs.astype(np.float32) / 255.0\n",
    "    obs = torch.from_numpy(obs).unsqueeze(0)\n",
    "    return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_observation(obs_batch):\n",
    "    # selecting the row from 35 to 195 so that we have a space of 160 * 160\n",
    "    obs_batch = obs_batch[:, 35:195, :, :]\n",
    "    # change to gray scale\n",
    "    obs_batch = np.dot(obs_batch, [0.2989, 0.5870, 0.1140])\n",
    "    # resize \n",
    "    obs_batch = np.transpose(obs_batch, (1, 2, 0))\n",
    "    obs_batch = cv2.resize(obs_batch, (64, 64), interpolation=cv2.INTER_AREA)\n",
    "    #obs_batch = obs_batch.astype(np.float32) / 255.0\n",
    "    #obs_batch = torch.from_numpy(obs_batch).unsqueeze(1)\n",
    "    return obs_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_observation(obs_batch):\n",
    "    obs_batch = obs_batch[:, 35:195, :, :]\n",
    "    obs_batch = cv2.resize(obs_batch, (64, 64), interpolation=cv2.INTER_AREA)\n",
    "    #obs_batch = obs_batch.astype(np.float32) / 255.0\n",
    "    #obs_batch = torch.from_numpy(obs_batch).permute(0, 3, 1, 2)\n",
    "    return obs_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_observation(obs_batch):\n",
    "    # create an empty array to hold the resized observations\n",
    "    resized_obs = np.empty((obs_batch.shape[0], 1, 64, 64), dtype=np.float32)\n",
    "\n",
    "    for i, obs in enumerate(obs_batch):\n",
    "        # selecting the row from 35 to 195 so that we have a space of 160 * 160\n",
    "        obs = obs[35:195, :, :]\n",
    "        # resize\n",
    "        obs = cv2.resize(obs, (64, 64), interpolation=cv2.INTER_AREA)\n",
    "        # convert to float32 and normalize\n",
    "        obs = obs.astype(np.float32) / 255.0\n",
    "        # transpose from (64, 64, 3) to (3, 64, 64) and add an extra dimension\n",
    "        obs = np.transpose(obs, (2, 0, 1))[np.newaxis, ...]\n",
    "        # add to the resized observations array\n",
    "        resized_obs[i] = obs\n",
    "\n",
    "    return resized_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_observation(obs_batch):\n",
    "    # selecting the row from 35 to 195 so that we have a space of 160 * 160\n",
    "    obs_batch = obs_batch[:, 35:195, :, :]\n",
    "    # resize \n",
    "    obs_batch = np.transpose(obs_batch, (0, 3, 1, 2))\n",
    "    obs_batch = np.asarray([cv2.resize(img, (64, 64), interpolation=cv2.INTER_AREA) for img in obs_batch])\n",
    "    obs_batch = obs_batch.astype(np.float32) / 255.0\n",
    "    obs_batch = torch.from_numpy(obs_batch).unsqueeze(1)\n",
    "    return obs_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_observation(obs_batch):\n",
    "    # Convert to float and rescale to range [0, 1]\n",
    "    #obs_batch = obs_batch.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Crop the score and border region\n",
    "    obs_batch = obs_batch[:, 35:195, :, :]\n",
    "    \n",
    "    # Resize to 64x64\n",
    "    transform = transforms.Resize((64, 64))\n",
    "    obs_batch = torch.stack([transform(obs.permute(2, 0, 1)) for obs in obs_batch])\n",
    "    \n",
    "    return obs_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_shape=(1, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_name)\n",
    "print(env.reset().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(env_name)\n",
    "obs = env.reset()\n",
    "print(obs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = envs.reset()\n",
    "print(obs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "envs = ParallelEnv([lambda: Environment(env_name).env for _ in range(num_envs)])\n",
    "print(obs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = preprocess_observation(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = resize_observation(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    #def plot_stats(self):\n",
    "    #    plt.plot(self.stats['Actor Loss'], label='Actor Loss')\n",
    "    #    plt.plot(self.stats['Critic Loss'], label='Critic Loss')\n",
    "    #    plt.plot(self.stats['Returns'], label='Returns')\n",
    "    #    plt.legend()\n",
    "    #    plt.show()\n",
    "\n",
    "    #def test_policy_network(self, env, episodes=2):\n",
    "    #    with torch.no_grad():\n",
    "    #        for episode in range(episodes):\n",
    "    #            state = env.reset()\n",
    "    #            done = False\n",
    "    #            total_reward = 0\n",
    "\n",
    "    #            while not done:\n",
    "    #                action = self.actor(state).argmax(dim=1).detach()\n",
    "    #                next_state, reward, done, _ = env.step(action)\n",
    "    #                total_reward += reward\n",
    "    #                state = next_state\n",
    "\n",
    "    #            print(f'Episode {episode+1}: Total reward = {total_reward.item()}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
