{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gym\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from IPython.core.debugger import set_trace\n",
    "import matplotlib.pyplot as plt\n",
    "import common.wrappers\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_frames = 5000000\n",
    "batch_size = 5\n",
    "learning_rate = 7e-4\n",
    "gamma = 0.99\n",
    "entropy_coef = 0.01\n",
    "critic_coef = 0.5\n",
    "env_name = 'PongNoFrameskip-v4'\n",
    "no_of_workers = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    FloatTensor = torch.cuda.FloatTensor\n",
    "    LongTensor = torch.cuda.LongTensor\n",
    "else:\n",
    "    FloatTensor = torch.FloatTensor\n",
    "    LongTensor = torch.LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = common.wrappers.make_atari(env_name)\n",
    "env = common.wrappers.wrap_deepmind(env, scale=True)\n",
    "env = common.wrappers.wrap_pytorch(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, action_space):\n",
    "        super(Model, self).__init__()\n",
    "        self.features = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 32, kernel_size=8, stride=4),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        feature_size = self.features(torch.zeros(1, *env.observation_space.shape)).cuda().view(1, -1).size(1)\n",
    "        # feature_size = self.features(torch.zeros(1, *env.observation_space.shape)).cpu().view(1, -1).size(1)\n",
    "        \n",
    "        self.critic = torch.nn.Sequential(\n",
    "            torch.nn.Linear(feature_size, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 1)\n",
    "        )\n",
    "        self.actor = torch.nn.Sequential(\n",
    "            torch.nn.Linear(feature_size, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, action_space),\n",
    "            torch.nn.Softmax(dim=-1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        value = self.critic(x)\n",
    "        actions = self.actor(x)\n",
    "        return value, actions\n",
    "    \n",
    "    def get_critic(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.critic(x)\n",
    "    \n",
    "    def evaluate_action(self, state, action):\n",
    "        value, actor_features = self.forward(state)\n",
    "        dist = torch.distributions.Categorical(actor_features)\n",
    "        \n",
    "        log_probs = dist.log_prob(action).view(-1, 1)\n",
    "        entropy = dist.entropy().mean()\n",
    "        \n",
    "        return value, log_probs, entropy\n",
    "    \n",
    "    def act(self, state):\n",
    "        value, actor_features = self.forward(state)\n",
    "        dist = torch.distributions.Categorical(actor_features)\n",
    "        \n",
    "        chosen_action = dist.sample()\n",
    "        return chosen_action.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(object):\n",
    "    def __init__(self):\n",
    "        self.states, self.actions, self.true_values = [], [], []\n",
    "    \n",
    "    def push(self, state, action, true_value):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.true_values.append(true_value)\n",
    "    \n",
    "    def pop_all(self):\n",
    "        states = torch.stack(self.states)\n",
    "        actions = LongTensor(self.actions)\n",
    "        true_values = FloatTensor(self.true_values).unsqueeze(1)\n",
    "        \n",
    "        self.states, self.actions, self.true_values = [], [], []\n",
    "        \n",
    "        return states, actions, true_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute true values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_true_values(states, rewards, dones):\n",
    "    R = []\n",
    "    rewards = FloatTensor(rewards)\n",
    "    dones = FloatTensor(dones)\n",
    "    states = torch.stack(states)\n",
    "    \n",
    "    if dones[-1] == True:\n",
    "        next_value = rewards[-1]\n",
    "    else:\n",
    "        next_value = model.get_critic(states[-1].unsqueeze(0))\n",
    "        \n",
    "    R.append(next_value)\n",
    "    for i in reversed(range(0, len(rewards) - 1)):\n",
    "        if not dones[i]:\n",
    "            next_value = rewards[i] + next_value * gamma\n",
    "        else:\n",
    "            next_value = rewards[i]\n",
    "        R.append(next_value)\n",
    "        \n",
    "    R.reverse()\n",
    "    \n",
    "    return FloatTensor(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflect(memory):\n",
    "    states, actions, true_values = memory.pop_all()\n",
    "\n",
    "    values, log_probs, entropy = model.evaluate_action(states, actions)\n",
    "\n",
    "    advantages =  true_values - values\n",
    "    critic_loss = advantages.pow(2).mean()\n",
    "\n",
    "    actor_loss = -(log_probs * advantages.detach()).mean()\n",
    "    total_loss = (critic_coef * critic_loss) + actor_loss - (entropy_coef * entropy)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "    optimizer.step()\n",
    "        \n",
    "    return values.mean().item()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker(object):\n",
    "    def __init__(self, env_name):\n",
    "        self.env = common.wrappers.make_atari(env_name)\n",
    "        self.env = common.wrappers.wrap_deepmind(self.env, scale=True)\n",
    "        self.env = common.wrappers.wrap_pytorch(self.env)\n",
    "        self.episode_reward = 0\n",
    "        self.state = FloatTensor(self.env.reset())\n",
    "        \n",
    "    def get_batch(self):\n",
    "        states, actions, rewards, dones = [], [], [], []\n",
    "        for _ in range(batch_size):\n",
    "            action = model.act(self.state.unsqueeze(0))\n",
    "            next_state, reward, done, _ = self.env.step(action)\n",
    "            self.episode_reward += reward\n",
    "\n",
    "            states.append(self.state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            dones.append(done)\n",
    "            \n",
    "            if done:\n",
    "                self.state = FloatTensor(self.env.reset())\n",
    "                data['episode_rewards'].append(self.episode_reward)\n",
    "                self.episode_reward = 0\n",
    "            else:\n",
    "                self.state = FloatTensor(next_state)\n",
    "                \n",
    "        values = compute_true_values(states, rewards, dones).unsqueeze(1)\n",
    "        return states, actions, values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(data, frame_idx):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    if data['episode_rewards']:\n",
    "        ax = plt.subplot(121)\n",
    "        ax = plt.gca()\n",
    "        average_score = np.mean(data['episode_rewards'][-100:])\n",
    "        plt.title(f\"Frame: {frame_idx} - Average Score: {average_score}\")\n",
    "        plt.grid()\n",
    "        plt.plot(data['episode_rewards'])\n",
    "    if data['values']:\n",
    "        ax = plt.subplot(122)\n",
    "        average_value = np.mean(data['values'][-1000:])\n",
    "        plt.title(f\"Frame: {frame_idx} - Average Values: {average_value}\")\n",
    "        plt.plot(data['values'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(env.action_space.n).cuda()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate, eps=1e-5)\n",
    "memory = Memory()\n",
    "workers = []\n",
    "for _ in range(no_of_workers):\n",
    "    workers.append(Worker(env_name))\n",
    "frame_idx = 0\n",
    "data = {\n",
    "    'episode_rewards': [],\n",
    "    'values': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug\n",
    "state = FloatTensor(env.reset())\n",
    "episode_reward = 0\n",
    "while frame_idx < max_frames:\n",
    "    for worker in workers:\n",
    "        states, actions, true_values = worker.get_batch()\n",
    "        for i, _ in enumerate(states):\n",
    "            memory.push(\n",
    "                states[i],\n",
    "                actions[i],\n",
    "                true_values[i]\n",
    "            )\n",
    "        frame_idx += batch_size\n",
    "        \n",
    "    value = reflect(memory)\n",
    "    if frame_idx % 1000 == 0:\n",
    "        data['values'].append(value)\n",
    "        plot(data, frame_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './weights/A2C-Pong.weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('./weights/A2C-Pong.weights'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
