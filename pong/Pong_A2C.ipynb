{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the necessary software libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Todos: \n",
    "# are we training everything: try for 500-1000\n",
    "# check that every layer in 3 networks, we have non-zero gradient (just a few updates)\n",
    "# check that the weight is changed after the update (just make sure the weight is changed for a few updates)\n",
    "# check that the actor action (histrogram of actor) check for a few episode\n",
    "# plot histrogram of the input, the state after preprocessing\n",
    "# change the hyper-parameter like lr, scale of the reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch import nn as nn\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import json\n",
    "\n",
    "from utils import test_policy_network, seed_everything, plot_stats\n",
    "from parallel_env import ParallelEnv, ParallelWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import ROM from the extracted file to atari_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run only once when creating conda environment\n",
    "! python -m atari_py.import_roms /Users/meng/Downloads/Roms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'Pong-v4'\n",
    "#env_name = 'Acrobot-v1'\n",
    "\n",
    "env = gym.make(env_name)\n",
    "\n",
    "input_space = env.observation_space.shape\n",
    "\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"State dimensions: {input_space}. Actions: {actions}\")\n",
    "print(f\"Sample state: {env.reset()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"state_space[0] = {input_space[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.real(env.render(mode='rgb_array')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "\n",
    "    def __init__(self, env_name):\n",
    "        self.env = gym.make(env_name)\n",
    "        seed_everything(self.env)\n",
    "\n",
    "    def reset(self):\n",
    "        obs = self.env.reset()\n",
    "        return obs\n",
    "\n",
    "    def step_async(self, actions):\n",
    "        actions = actions.squeeze().numpy()\n",
    "        self.env.step_async(actions)\n",
    "\n",
    "    def step_wait(self):\n",
    "        obs, reward, done, info = self.env.step_wait()\n",
    "        obs = self.preprocess_observation(obs)\n",
    "        reward = torch.tensor(reward).unsqueeze(1).float()\n",
    "        done = torch.tensor(done).unsqueeze(1)\n",
    "        return obs, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessEnv(ParallelWrapper):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def reset(self):\n",
    "        state = self.venv.reset()\n",
    "        return torch.from_numpy(state).float()\n",
    "\n",
    "    def step_async(self, actions):\n",
    "        actions = actions.squeeze().numpy()\n",
    "        self.venv.step_async(actions)\n",
    "\n",
    "    def step_wait(self):\n",
    "        next_state, reward, done, info = self.venv.step_wait()\n",
    "        next_state = torch.from_numpy(next_state).float()\n",
    "        reward = torch.tensor(reward).unsqueeze(1).float()\n",
    "        done = torch.tensor(done).unsqueeze(1)\n",
    "        return next_state, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "\n",
    "    def __init__(self, input_shape):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv4 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64*4*4, 512)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        \n",
    "        x = x.view(-1, 64*4*4)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "\n",
    "    def __init__(self, n_actions):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=512, out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=512, out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic():\n",
    "    def __init__(self, actor, critic, feature, alpha=1e-4, gamma=0.99, path=\"best_model.pth.tar\"):\n",
    "        self.actor = actor\n",
    "        self.critic = critic\n",
    "        self.feature = feature\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.actor_optim = AdamW(self.actor.parameters(), lr=1e-3)\n",
    "        self.critic_optim = AdamW(self.critic.parameters(), lr=1e-4)\n",
    "        self.feature_optim = AdamW(self.feature.parameters(), lr=1e-4)\n",
    "        self.stats = {'Actor Loss': [], 'Critic Loss': [], 'Returns': []}\n",
    "        self.best_return = -9999\n",
    "        self.path = path\n",
    "        \n",
    "    def preprocess_observation(self, obs_batch):\n",
    "        # Crop the score and border region\n",
    "        obs_batch = obs_batch[:, 35:195, :, :]\n",
    "    \n",
    "        # Resize to 64x64\n",
    "        transform = transforms.Resize((64, 64))\n",
    "        \n",
    "        obs_batch = torch.stack([transform(obs.permute(2, 0, 1)) for obs in obs_batch])\n",
    "        \n",
    "        # Convert to float and rescale to range [0, 1]\n",
    "        obs_batch = obs_batch / 255.0\n",
    "        \n",
    "        return obs_batch\n",
    "\n",
    "    def train(self, env, episodes):\n",
    "        for episode in tqdm(range(1, episodes + 1)):\n",
    "            state = env.reset()\n",
    "            state = self.preprocess_observation(state)\n",
    "            \n",
    "            done_b = torch.zeros((env.num_envs, 1), dtype=torch.bool)\n",
    "            ep_return = torch.zeros((env.num_envs, 1))\n",
    "            I = 1.\n",
    "\n",
    "            while not done_b.all():\n",
    "                \n",
    "                self.critic_optim.zero_grad()\n",
    "                self.actor_optim.zero_grad()\n",
    "\n",
    "                state_f = self.feature(state)\n",
    "                probs = self.actor(state_f)\n",
    "                \n",
    "                action = torch.multinomial(probs, 1).squeeze().detach()\n",
    "                \n",
    "\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                next_state = self.preprocess_observation(next_state)\n",
    "                \n",
    "                value = self.critic(state_f)\n",
    "                next_state_f = self.feature(next_state)\n",
    "                \n",
    "                target = reward + ~done * self.gamma * self.critic(next_state_f).detach()\n",
    "                critic_loss = nn.functional.mse_loss(value, target)\n",
    "                \n",
    "                \n",
    "                advantage = (target - value).detach()\n",
    "                #probs = self.actor(state)\n",
    "                log_probs = torch.log(probs + 1e-6)\n",
    "                \n",
    "                action = action.view(-1, 1)\n",
    "                \n",
    "                action_log_prob = log_probs.gather(1, action)\n",
    "                entropy = - torch.sum(probs * log_probs, dim=-1, keepdim=True)\n",
    "                actor_loss = - I * action_log_prob * advantage - 0.01 * entropy\n",
    "                actor_loss = actor_loss.mean()\n",
    "                \n",
    "                total_loss = actor_loss + critic_loss\n",
    "                \n",
    "                total_loss.backward()\n",
    "                self.feature_optim.step()\n",
    "                self.critic_optim.step()\n",
    "                self.actor_optim.step()\n",
    "\n",
    "                ep_return += reward\n",
    "                done_b |= done\n",
    "                state = next_state\n",
    "                I = I * self.gamma\n",
    "\n",
    "            \n",
    "            current_return = ep_return.mean().item()\n",
    "            if self.best_return < current_return:\n",
    "                best_return = current_return\n",
    "                torch.save({\n",
    "                    'actor_state_dict': self.actor.state_dict(),\n",
    "                    'critic_state_dict': self.critic.state_dict(),\n",
    "                    'feature_state_dict': self.feature.state_dict(),\n",
    "                    'actor_optim_state_dict': self.actor_optim.state_dict(),\n",
    "                    'critic_optim_state_dict': self.critic_optim.state_dict(),\n",
    "                    'feature_optim_state_dict': self.feature_optim.state_dict(),\n",
    "                }, self.path)\n",
    "            \n",
    "            \n",
    "            self.stats['Actor Loss'].append(actor_loss.item())\n",
    "            self.stats['Critic Loss'].append(critic_loss.item())\n",
    "            self.stats['Returns'].append(ep_return.mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'Pong-v4'\n",
    "num_envs = os.cpu_count()\n",
    "episodes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = ParallelEnv([lambda: Environment(env_name).env for _ in range(num_envs)])\n",
    "envs = PreprocessEnv(envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = FeatureExtractor(envs.observation_space.shape)\n",
    "actor = Actor(envs.action_space.n)\n",
    "critic = Critic()\n",
    "agent = ActorCritic(actor, critic, feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train(envs, episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stats(agent.stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_name)\n",
    "test_policy_network_internal(env, agent.actor, agent.feature_extractor, episodes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_policy_network_internal(env, policy, feature_extractor, episodes=1):\n",
    "    from IPython import display\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        state = preprocess(state)\n",
    "        state = agent.feature_extractor(state)\n",
    "        done = False\n",
    "        img = plt.imshow(env.render(mode='rgb_array'))\n",
    "        while not done:\n",
    "            # state = torch.from_numpy(state).unsqueeze(0).float()\n",
    "            action = policy(state).multinomial(1).item()\n",
    "            next_state, _, done, _ = env.step(action)\n",
    "            img.set_data(env.render(mode='rgb_array'))\n",
    "            plt.axis('off')\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "            state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(obs):\n",
    "        # Convert to float and rescale to range [0, 1]\n",
    "        #obs_batch = obs_batch.astype(np.float32) / 255.0\n",
    "        \n",
    "        obs = torch.from_numpy(obs)\n",
    "    \n",
    "        # Crop the score and border region\n",
    "        obs = obs[35:195, :, :]\n",
    "    \n",
    "        # Resize to 64x64\n",
    "        transform = transforms.Resize((64, 64))\n",
    "        obs = transform(obs.permute(2, 0, 1))\n",
    "        obs = obs.float()\n",
    "    \n",
    "        return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/meng/Documents/semester8/Thesis/workspace/pong_and_a2c/data2/meng/stats.txt', 'r') as f:\n",
    "    # Load the contents of the file as a JSON object\n",
    "    stats_loaded = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stats(stats_loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(obs):\n",
    "    # Crop the score and border region\n",
    "    obs = obs[35:195, :, :]\n",
    "    \n",
    "    # Resize to 64x64\n",
    "    transform = transforms.Resize((64, 64))\n",
    "        \n",
    "    # obs = torch.stack([transform(obs.permute(2, 0, 1)) for obs in obs_batch])\n",
    "    # obs = transform(obs.permute(2, 0, 1))\n",
    "        \n",
    "    # Convert to float and rescale to range [0, 1]\n",
    "    # obs = obs / 255.0\n",
    "        \n",
    "    return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pong-v0')\n",
    "env.unwrapped.ball_size = 5  # Set the ball size to 5 pixels\n",
    "env.unwrapped.ball_color = (255, 255, 255)  # Set the ball color to white\n",
    "env.reset()\n",
    "\n",
    "# Play one step of the game to get an image with the ball\n",
    "obs, _, _, _ = env.step(0)\n",
    "\n",
    "# Render the environment\n",
    "img = env.render(mode='rgb_array')\n",
    "# img = env.render(mode='rgb_array', viewer='pyglet')\n",
    "\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = preprocess(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_policy_network(env, , episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = feature_extractor(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = actor(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_state(env, policy, episodes=1):\n",
    "    from IPython import display\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        img = plt.imshow(env.render(mode='rgb_array'))\n",
    "        while not done:\n",
    "            state = torch.from_numpy(state).unsqueeze(0).float()\n",
    "            action = policy(state).multinomial(1).item()\n",
    "            next_state, _, done, _ = env.step(action)\n",
    "            img.set_data(env.render(mode='rgb_array'))\n",
    "            plt.axis('off')\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "            state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_policy_network(env, policy, episodes=1):\n",
    "    from IPython import display\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        img = plt.imshow(env.render(mode='rgb_array'))\n",
    "        while not done:\n",
    "            state = torch.from_numpy(state).unsqueeze(0).float()\n",
    "            action = policy(state).multinomial(1).item()\n",
    "            next_state, _, done, _ = env.step(action)\n",
    "            img.set_data(env.render(mode='rgb_array'))\n",
    "            plt.axis('off')\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "            state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_observation(obs_batch):\n",
    "    # selecting the row from 35 to 195 so that we have a space of 160 * 160\n",
    "    obs_batch = obs_batch[:, 35:195, :, :]\n",
    "    # change to gray scale\n",
    "    obs_batch = np.dot(obs_batch, [0.2989, 0.5870, 0.1140])\n",
    "    # resize \n",
    "    obs_batch = np.transpose(obs_batch, (1, 2, 0))\n",
    "    obs_batch = cv2.resize(obs_batch, (64, 64), interpolation=cv2.INTER_AREA)\n",
    "    #obs_batch = obs_batch.astype(np.float32) / 255.0\n",
    "    #obs_batch = torch.from_numpy(obs_batch).unsqueeze(1)\n",
    "    return obs_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_observation(obs_batch):\n",
    "    obs_batch = obs_batch[:, 35:195, :, :]\n",
    "    obs_batch = cv2.resize(obs_batch, (64, 64), interpolation=cv2.INTER_AREA)\n",
    "    #obs_batch = obs_batch.astype(np.float32) / 255.0\n",
    "    #obs_batch = torch.from_numpy(obs_batch).permute(0, 3, 1, 2)\n",
    "    return obs_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_observation(obs_batch):\n",
    "    # create an empty array to hold the resized observations\n",
    "    resized_obs = np.empty((obs_batch.shape[0], 1, 64, 64), dtype=np.float32)\n",
    "\n",
    "    for i, obs in enumerate(obs_batch):\n",
    "        # selecting the row from 35 to 195 so that we have a space of 160 * 160\n",
    "        obs = obs[35:195, :, :]\n",
    "        # resize\n",
    "        obs = cv2.resize(obs, (64, 64), interpolation=cv2.INTER_AREA)\n",
    "        # convert to float32 and normalize\n",
    "        obs = obs.astype(np.float32) / 255.0\n",
    "        # transpose from (64, 64, 3) to (3, 64, 64) and add an extra dimension\n",
    "        obs = np.transpose(obs, (2, 0, 1))[np.newaxis, ...]\n",
    "        # add to the resized observations array\n",
    "        resized_obs[i] = obs\n",
    "\n",
    "    return resized_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_observation(obs_batch):\n",
    "    # selecting the row from 35 to 195 so that we have a space of 160 * 160\n",
    "    obs_batch = obs_batch[:, 35:195, :, :]\n",
    "    # resize \n",
    "    obs_batch = np.transpose(obs_batch, (0, 3, 1, 2))\n",
    "    obs_batch = np.asarray([cv2.resize(img, (64, 64), interpolation=cv2.INTER_AREA) for img in obs_batch])\n",
    "    obs_batch = obs_batch.astype(np.float32) / 255.0\n",
    "    obs_batch = torch.from_numpy(obs_batch).unsqueeze(1)\n",
    "    return obs_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_observation(obs_batch):\n",
    "    # Convert to float and rescale to range [0, 1]\n",
    "    #obs_batch = obs_batch.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Crop the score and border region\n",
    "    obs_batch = obs_batch[:, 35:195, :, :]\n",
    "    \n",
    "    # Resize to 64x64\n",
    "    transform = transforms.Resize((64, 64))\n",
    "    obs_batch = torch.stack([transform(obs.permute(2, 0, 1)) for obs in obs_batch])\n",
    "    \n",
    "    return obs_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_shape=(1, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_name)\n",
    "print(env.reset().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(env_name)\n",
    "obs = env.reset()\n",
    "print(obs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = envs.reset()\n",
    "print(obs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "envs = ParallelEnv([lambda: Environment(env_name).env for _ in range(num_envs)])\n",
    "print(obs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = preprocess_observation(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = resize_observation(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    #def plot_stats(self):\n",
    "    #    plt.plot(self.stats['Actor Loss'], label='Actor Loss')\n",
    "    #    plt.plot(self.stats['Critic Loss'], label='Critic Loss')\n",
    "    #    plt.plot(self.stats['Returns'], label='Returns')\n",
    "    #    plt.legend()\n",
    "    #    plt.show()\n",
    "\n",
    "    #def test_policy_network(self, env, episodes=2):\n",
    "    #    with torch.no_grad():\n",
    "    #        for episode in range(episodes):\n",
    "    #            state = env.reset()\n",
    "    #            done = False\n",
    "    #            total_reward = 0\n",
    "\n",
    "    #            while not done:\n",
    "    #                action = self.actor(state).argmax(dim=1).detach()\n",
    "    #                next_state, reward, done, _ = env.step(action)\n",
    "    #                total_reward += reward\n",
    "    #                state = next_state\n",
    "\n",
    "    #            print(f'Episode {episode+1}: Total reward = {total_reward.item()}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
